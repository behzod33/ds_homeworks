{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba5ac40f",
   "metadata": {},
   "source": [
    "\n",
    "# Экзаменационные вопросы по машинному обучению\n",
    "\n",
    "## 1. Что такое машинное обучение и искусственный интеллект?\n",
    "- Определение искусственного интеллекта (ИИ) и машинного обучения (МО).\n",
    "- Основные принципы работы.\n",
    "- Виды обучения: с учителем и без учителя.\n",
    "\n",
    "## 2. Задача регрессии\n",
    "- Основная идея задачи регрессии.\n",
    "- Какие модели можно использовать для её решения.\n",
    "- Отличие регрессии от задачи классификации.\n",
    "- Примеры задач регрессии из реальной жизни.\n",
    "\n",
    "## 3. Линейная регрессия\n",
    "- Принципы построения модели.\n",
    "- Особенности метода.\n",
    "- Преимущества и недостатки линейной регрессии.\n",
    "\n",
    "## 4. Метрики для задачи регрессии\n",
    "- Формулы основных метрик:\n",
    "  - Среднеквадратичная ошибка (MSE).\n",
    "  - Средняя абсолютная ошибка (MAE).\n",
    "  - Коэффициент детерминации (R²).\n",
    "  - Корень среднеквадратичной ошибки (RMSE).\n",
    "  - Средняя абсолютная процентная ошибка (MAPE).\n",
    "- Различия между метриками и их применение.\n",
    "\n",
    "## 5. Метод ближайших соседей (KNN)\n",
    "- Принципы работы метода для задач регрессии и классификации.\n",
    "- Преимущества и недостатки метода.\n",
    "- Гиперпараметры и их интерпретация (n_neighbors, weights, metric).\n",
    "\n",
    "## 6. Методы кодирования категориальных данных\n",
    "- Основные методы:\n",
    "  - One-Hot Encoding.\n",
    "  - Label Encoding.\n",
    "  - Target Encoding.\n",
    "- Принципы работы каждого метода.\n",
    "- Преимущества и недостатки.\n",
    "\n",
    "## 7. Этапы подготовки данных перед обучением моделей\n",
    "- Проверка качества данных (наличие пропусков, выбросов).\n",
    "- Нормализация и стандартизация.\n",
    "- Проверка корреляций между признаками.\n",
    "\n",
    "## 8. Задача классификации\n",
    "- Основная идея задачи классификации.\n",
    "- Отличие от задачи регрессии.\n",
    "- Примеры задач классификации из реальной жизни.\n",
    "\n",
    "## 9. Логистическая регрессия\n",
    "- Отличие от линейной регрессии.\n",
    "- Формулы и алгоритм построения модели.\n",
    "- Преимущества и недостатки логистической регрессии.\n",
    "\n",
    "## 10. Метрики для задач классификации\n",
    "- Основные метрики:\n",
    "  - Точность (Accuracy).\n",
    "  - Полнота (Recall).\n",
    "  - Точность (Precision).\n",
    "  - F1-мера (F1-score).\n",
    "- Отличия и особенности каждой метрики.\n",
    "- Рекомендации по выбору метрик в зависимости от задачи.\n",
    "\n",
    "## 11. Матрица ошибок (Confusion Matrix)\n",
    "- Способ построения матрицы.\n",
    "- Применение в задачах классификации.\n",
    "\n",
    "## 12. AUC-ROC\n",
    "- Принципы построения ROC-кривой.\n",
    "- Значение метрики AUC и её особенности.\n",
    "- Применение для оценки моделей.\n",
    "\n",
    "## 13. Методы валидации\n",
    "- Разделение на обучающую и тестовую выборки: пропорции и причины.\n",
    "- Кросс-валидация: что это такое и как работает. Разновидности.\n",
    "\n",
    "## 14. Недообучение, переобучение\n",
    "- Как обнаружить.\n",
    "- Способы предотвращения.\n",
    "\n",
    "## 15. Градиентный спуск\n",
    "- Объяснение алгоритма.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d58faef",
   "metadata": {},
   "source": [
    "# Ответы на экзаменационные вопросы по машинному обучению\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Что такое машинное обучение и искусственный интеллект?\n",
    "\n",
    "### Определение искусственного интеллекта (ИИ) и машинного обучения (МО)\n",
    "\n",
    "**Искусственный интеллект (ИИ)** – это область компьютерных наук, занимающаяся созданием систем, которые могут выполнять задачи, требующие человеческого интеллекта, такие как обработка естественного языка, распознавание изображений, принятие решений и автономное управление.\n",
    "\n",
    "**Машинное обучение (МО)** – это подраздел ИИ, который использует алгоритмы для выявления закономерностей в данных и принятия решений без явного программирования. МО работает на основе статистических методов и позволяет компьютерам учиться на основе данных.\n",
    "\n",
    "### Основные принципы работы\n",
    "\n",
    "1. **Обучение на данных** – алгоритмы машинного обучения анализируют большие объемы данных, чтобы выявить закономерности и зависимости.\n",
    "2. **Обобщение** – модель должна уметь делать предсказания на новых данных, которые она не видела во время обучения.\n",
    "3. **Адаптация** – алгоритмы могут дообучаться и корректироваться по мере поступления новых данных.\n",
    "\n",
    "### Виды обучения\n",
    "\n",
    "1. **Обучение с учителем (Supervised Learning)**  \n",
    "   - В обучающей выборке есть входные данные и правильные ответы (метки классов или числовые значения).\n",
    "   - Примеры:  \n",
    "     - Классификация (определение, является ли письмо спамом или нет).  \n",
    "     - Регрессия (предсказание стоимости недвижимости).\n",
    "\n",
    "2. **Обучение без учителя (Unsupervised Learning)**  \n",
    "   - В обучающей выборке есть только входные данные без разметки.\n",
    "   - Алгоритм самостоятельно находит закономерности и структуры в данных.\n",
    "   - Примеры:  \n",
    "     - Кластеризация (группировка пользователей по схожим интересам).\n",
    "     - Уменьшение размерности данных.\n",
    "\n",
    "3. **Обучение с подкреплением (Reinforcement Learning)**  \n",
    "   - Агент взаимодействует с окружающей средой и получает награды за правильные действия.\n",
    "   - Применяется в играх, робототехнике и управлении системами.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Задача регрессии\n",
    "\n",
    "### Основная идея задачи регрессии\n",
    "\n",
    "**Регрессия** – это тип задачи машинного обучения, в которой предсказывается **непрерывная числовая переменная** на основе входных данных.\n",
    "\n",
    "Пример:  \n",
    "- Предсказание температуры воздуха по данным о влажности, скорости ветра и времени суток.\n",
    "- Прогнозирование спроса на товар в зависимости от сезона.\n",
    "\n",
    "### Какие модели можно использовать для её решения?\n",
    "\n",
    "1. **Линейная регрессия**  \n",
    "   - Основная модель, использующая линейную зависимость между входными данными и выходной переменной.\n",
    "   - Уравнение:  \n",
    "     $$\n",
    "     y = b_0 + b_1 x_1 + b_2 x_2 + ... + b_n x_n\n",
    "     $$\n",
    "   - Простая интерпретация, но не всегда применима к сложным данным.\n",
    "\n",
    "2. **Полиномиальная регрессия**  \n",
    "   - Расширенная версия линейной регрессии, где добавляются степени входных переменных.\n",
    "   - Позволяет моделировать нелинейные зависимости.\n",
    "\n",
    "3. **Методы на основе деревьев решений (Decision Trees, Random Forest)**  \n",
    "   - Деревья решений разбивают пространство признаков на подгруппы и делают предсказания на основе правил.\n",
    "   - Хорошо работают с разнородными данными, но могут переобучаться.\n",
    "\n",
    "4. **Градиентный бустинг (XGBoost, LightGBM)**  \n",
    "   - Продвинутая модель на основе ансамблей деревьев решений.\n",
    "   - Используется в соревнованиях по анализу данных из-за высокой точности.\n",
    "\n",
    "5. **Нейронные сети**  \n",
    "   - Глубокие модели, имитирующие работу мозга.\n",
    "   - Используются для сложных регрессионных задач, таких как прогнозирование финансовых временных рядов.\n",
    "\n",
    "### Отличие регрессии от задачи классификации\n",
    "\n",
    "| Характеристика     | Регрессия | Классификация |\n",
    "|--------------------|----------|--------------|\n",
    "| Выходная переменная | Непрерывное число (например, цена) | Категория (например, \"да\" или \"нет\") |\n",
    "| Основные модели | Линейная регрессия, полиномиальная регрессия | Логистическая регрессия, деревья решений |\n",
    "| Пример | Прогнозирование температуры | Определение спам-писем |\n",
    "\n",
    "### Примеры задач регрессии из реальной жизни\n",
    "\n",
    "1. **Финансы** – предсказание стоимости акций на основе исторических данных.\n",
    "2. **Медицина** – прогнозирование уровня сахара в крови на основе анализа образа жизни пациента.\n",
    "3. **Бизнес** – прогнозирование спроса на продукцию в зависимости от сезона.\n",
    "4. **Интернет-маркетинг** – прогнозирование вероятности покупки товара пользователем.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Линейная регрессия\n",
    "\n",
    "### Принципы построения модели\n",
    "\n",
    "Линейная регрессия предполагает, что зависимость между входными переменными и целевой переменной можно описать линейной функцией:\n",
    "\n",
    "$$\n",
    "y = b_0 + b_1 x_1 + b_2 x_2 + ... + b_n x_n + \\varepsilon\n",
    "$$\n",
    "\n",
    "где:\n",
    "- \\\\( y \\\\) – предсказываемая переменная,\n",
    "- \\\\( x_1, x_2, ..., x_n \\\\) – входные признаки,\n",
    "- \\\\( b_0 \\\\) – свободный член (intercept),\n",
    "- \\\\( b_1, b_2, ..., b_n \\\\) – коэффициенты модели,\n",
    "- \\\\( \\varepsilon \\\\) – случайная ошибка.\n",
    "\n",
    "### Особенности метода\n",
    "\n",
    "1. **Простота и интерпретируемость** – легко объяснить, какие факторы влияют на предсказание.\n",
    "2. **Чувствительность к выбросам** – сильные выбросы могут значительно изменить коэффициенты модели.\n",
    "3. **Линейное предположение** – плохо работает, если связь между переменными нелинейная.\n",
    "\n",
    "### Преимущества линейной регрессии\n",
    "\n",
    "- Легко интерпретируется.\n",
    "- Быстро обучается.\n",
    "- Хорошо работает на небольших наборах данных.\n",
    "- Позволяет делать статистический анализ значимости признаков.\n",
    "\n",
    "### Недостатки линейной регрессии\n",
    "\n",
    "- Плохо справляется с нелинейными зависимостями.\n",
    "- Подвержена выбросам и шуму в данных.\n",
    "- Требует нормализации данных для правильной работы.\n",
    "\n",
    "### Улучшения модели линейной регрессии\n",
    "\n",
    "1. **Полиномиальные признаки** – добавление степенных признаков для учета нелинейности.\n",
    "2. **Регуляризация (L1, L2)** – уменьшает переобучение за счет штрафов на большие коэффициенты.\n",
    "3. **Признаки взаимодействия** – добавление новых переменных, основанных на комбинации существующих.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07df522",
   "metadata": {},
   "source": [
    "## 4. Метрики для задачи регрессии\n",
    "\n",
    "### Зачем нужны метрики?\n",
    "Метрики качества используются для оценки точности модели регрессии. Они помогают определить, насколько предсказания модели соответствуют реальным данным.\n",
    "\n",
    "### Основные метрики регрессии:\n",
    "\n",
    "#### 1. **Среднеквадратичная ошибка (MSE – Mean Squared Error)**\n",
    "- Формула:\n",
    "  $$\n",
    "  MSE = \\frac{1}{n} \\sum (y_i - \\hat{y_i})^2\n",
    "  $$\n",
    "- **Описание**: Вычисляет средний квадрат разницы между реальными значениями \\\\( y_i \\\\) и предсказанными \\\\( \\hat{y_i} \\\\).\n",
    "- **Плюсы**: Простота вычисления.\n",
    "- **Минусы**: Чувствительность к выбросам (из-за квадрата разности).\n",
    "\n",
    "#### 2. **Средняя абсолютная ошибка (MAE – Mean Absolute Error)**\n",
    "- Формула:\n",
    "  $$\n",
    "  MAE = \\frac{1}{n} \\sum |y_i - \\hat{y_i}|\n",
    "  $$\n",
    "- **Описание**: Среднее абсолютное отклонение предсказанных значений от фактических.\n",
    "- **Плюсы**: Более устойчив к выбросам, чем MSE.\n",
    "- **Минусы**: Может быть менее чувствителен к большим ошибкам.\n",
    "\n",
    "#### 3. **Корень среднеквадратичной ошибки (RMSE – Root Mean Squared Error)**\n",
    "- Формула:\n",
    "  $$\n",
    "  RMSE = \\sqrt{MSE}\n",
    "  $$\n",
    "- **Описание**: Берет квадратный корень из MSE, приводя ошибку к тем же единицам измерения, что и исходные данные.\n",
    "- **Плюсы**: Легче интерпретировать, чем MSE.\n",
    "- **Минусы**: Все еще чувствителен к выбросам.\n",
    "\n",
    "#### 4. **Коэффициент детерминации (R² – R-Squared)**\n",
    "- Формула:\n",
    "  $$\n",
    "  R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n",
    "  $$\n",
    "  где:\n",
    "  - \\\\( SS_{res} \\\\) – сумма квадратов отклонений предсказанных значений от фактических.\n",
    "  - \\\\( SS_{tot} \\\\) – сумма квадратов отклонений фактических значений от среднего.\n",
    "\n",
    "- **Описание**: Показывает, какую долю дисперсии данных объясняет модель.\n",
    "- **Плюсы**: Хорошая мера для интерпретации модели.\n",
    "- **Минусы**: Может быть отрицательным, если модель очень плоха.\n",
    "\n",
    "#### 5. **Средняя абсолютная процентная ошибка (MAPE – Mean Absolute Percentage Error)**\n",
    "- Формула:\n",
    "  $$\n",
    "  MAPE = \\frac{1}{n} \\sum \\left| \\frac{y_i - \\hat{y_i}}{y_i} \\right| \\times 100\\%\n",
    "  $$\n",
    "- **Описание**: Выражает среднюю ошибку в процентах.\n",
    "- **Плюсы**: Позволяет сравнивать модели между разными задачами.\n",
    "- **Минусы**: Проблемы с нулевыми значениями \\\\( y_i \\\\).\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Метод ближайших соседей (KNN)\n",
    "\n",
    "### Принципы работы метода\n",
    "**Метод K-ближайших соседей (KNN)** – это **непараметрический алгоритм**, который предсказывает значение целевой переменной на основе ближайших точек из обучающей выборки.\n",
    "\n",
    "**Как работает?**\n",
    "1. Выбираем количество соседей **\\\\( k \\\\)**.\n",
    "2. Для нового примера находим **\\\\( k \\\\)** ближайших точек в обучающей выборке.\n",
    "3. Если **задача регрессии**:\n",
    "   - Вычисляем среднее значение среди ближайших соседей.\n",
    "4. Если **задача классификации**:\n",
    "   - Выбираем класс, который встречается чаще среди соседей.\n",
    "\n",
    "### Преимущества метода:\n",
    "✅ **Простота в реализации** – легко объяснить и реализовать.  \n",
    "✅ **Нет необходимости в обучении** – предсказания делаются на лету.  \n",
    "✅ **Гибкость** – работает для задач регрессии и классификации.\n",
    "\n",
    "### Недостатки метода:\n",
    "❌ **Медленный при больших данных** – нужно сравнивать новый объект со всеми точками.  \n",
    "❌ **Чувствительность к шуму** – выбросы могут сильно влиять на результат.  \n",
    "❌ **Зависимость от метрики расстояния** – разные способы измерения расстояний дают разные результаты.\n",
    "\n",
    "### Гиперпараметры метода:\n",
    "- **\\\\( k \\\\)** – количество соседей (чем больше, тем сглаженнее предсказания).\n",
    "- **Метрика расстояния**:\n",
    "  - **Евклидово расстояние**:  \n",
    "    $$\n",
    "    d(x, y) = \\sqrt{\\sum (x_i - y_i)^2}\n",
    "    $$\n",
    "  - **Манхэттенское расстояние**:\n",
    "    $$\n",
    "    d(x, y) = \\sum |x_i - y_i|\n",
    "    $$\n",
    "- **Вес соседей**:\n",
    "  - Равные (Uniform) – все соседи вносят одинаковый вклад.\n",
    "  - Взвешенные (Distance-based) – ближние соседи важнее дальних.\n",
    "\n",
    "---\n",
    "\n",
    "## 6. Методы кодирования категориальных данных\n",
    "\n",
    "### Почему необходимо кодирование?\n",
    "Машинное обучение работает только с числовыми данными, но многие признаки могут быть **категориальными** (например, \"красный\", \"зеленый\", \"синий\"). Чтобы использовать их в моделях, необходимо преобразовать их в числовой формат.\n",
    "\n",
    "### Основные методы кодирования\n",
    "\n",
    "#### 1. **One-Hot Encoding**\n",
    "- Создает **бинарные (0 и 1) переменные** для каждого уникального значения.\n",
    "- Пример:\n",
    "\n",
    "  | Цвет  | One-Hot Encoding |\n",
    "  |-------|----------------|\n",
    "  | Красный  | 1 0 0 |\n",
    "  | Зеленый  | 0 1 0 |\n",
    "  | Синий    | 0 0 1 |\n",
    "\n",
    "- **Плюсы**:  \n",
    "  ✅ Не вводит порядковую зависимость.  \n",
    "  ✅ Работает с большинством алгоритмов.  \n",
    "\n",
    "- **Минусы**:  \n",
    "  ❌ Увеличивает размерность данных, если много уникальных значений.  \n",
    "\n",
    "#### 2. **Label Encoding**\n",
    "- Присваивает каждой категории числовое значение.\n",
    "- Пример:\n",
    "\n",
    "  | Цвет  | Label Encoding |\n",
    "  |-------|----------------|\n",
    "  | Красный  | 0 |\n",
    "  | Зеленый  | 1 |\n",
    "  | Синий    | 2 |\n",
    "\n",
    "- **Плюсы**:  \n",
    "  ✅ Простота.  \n",
    "\n",
    "- **Минусы**:  \n",
    "  ❌ Вводит ложный порядок между категориями.\n",
    "\n",
    "#### 3. **Target Encoding (Mean Encoding)**\n",
    "- Заменяет категорию средним значением целевого признака.\n",
    "- Пример:\n",
    "\n",
    "  | Город  | Средняя зарплата |\n",
    "  |--------|----------------|\n",
    "  | Москва  | 100 000 |\n",
    "  | Санкт-Петербург | 80 000 |\n",
    "  | Казань  | 60 000 |\n",
    "\n",
    "- **Плюсы**:  \n",
    "  ✅ Хорошо работает с моделями градиентного бустинга.  \n",
    "\n",
    "- **Минусы**:  \n",
    "  ❌ Может привести к **утечке данных**, если не используется кросс-валидация.  \n",
    "\n",
    "---\n",
    "\n",
    "## 7. Этапы подготовки данных перед обучением моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae40868",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Почему важна подготовка данных?\n",
    "Качество данных напрямую влияет на качество модели. Если данные содержат пропущенные значения, выбросы или несбалансированные классы, модель может работать некорректно.\n",
    "\n",
    "### Основные этапы подготовки данных:\n",
    "\n",
    "#### 1. **Проверка качества данных**\n",
    "Перед тем как передавать данные в модель, нужно убедиться, что в них нет проблем. Основные аспекты:\n",
    "- **Пропущенные значения** – строки или столбцы, где отсутствуют данные.\n",
    "- **Выбросы** – аномально большие или малые значения.\n",
    "- **Несбалансированные классы** – если один класс встречается намного чаще другого.\n",
    "\n",
    "#### 2. **Заполнение пропущенных значений**\n",
    "Способы обработки:\n",
    "- **Удаление строк или столбцов** – если пропусков мало.\n",
    "- **Заполнение средним/медианой** – для числовых данных.\n",
    "- **Заполнение модой (наиболее частым значением)** – для категориальных данных.\n",
    "- **Импутация (предсказание недостающих значений)** – использование моделей для заполнения пропусков.\n",
    "\n",
    "#### 3. **Нормализация и стандартизация**\n",
    "Многие алгоритмы требуют, чтобы данные были приведены к одинаковому масштабу.\n",
    "\n",
    "- **Нормализация (Min-Max Scaling)**  \n",
    "  Приводит значения в диапазон от 0 до 1:  \n",
    "  $$\n",
    "  X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}\n",
    "  $$\n",
    "  - Используется для методов, чувствительных к масштабу (например, KNN, SVM).\n",
    "  \n",
    "- **Стандартизация (Z-score Scaling)**  \n",
    "  Приводит данные к среднему значению 0 и стандартному отклонению 1:  \n",
    "  $$\n",
    "  X_{std} = \\frac{X - \\mu}{\\sigma}\n",
    "  $$\n",
    "  - Используется в линейной регрессии, логистической регрессии.\n",
    "\n",
    "#### 4. **Проверка корреляций между признаками**\n",
    "Если признаки сильно коррелированы, они несут дублирующую информацию. Методы обработки:\n",
    "- **Удаление одного из коррелированных признаков**.\n",
    "- **Главные компоненты (PCA)** – уменьшение размерности данных.\n",
    "\n",
    "---\n",
    "\n",
    "## 8. Задача классификации\n",
    "\n",
    "### Основная идея задачи классификации\n",
    "**Классификация** – это задача машинного обучения, в которой предсказывается **категориальная переменная** на основе входных данных.\n",
    "\n",
    "Пример:  \n",
    "- Определение, является ли письмо спамом или нет.\n",
    "- Классификация изображений (кошка или собака).\n",
    "\n",
    "### Отличие от задачи регрессии\n",
    "\n",
    "| Характеристика     | Классификация | Регрессия |\n",
    "|--------------------|--------------|----------|\n",
    "| Выходная переменная | Категория (например, \"да\" или \"нет\") | Непрерывное число (например, цена) |\n",
    "| Основные модели | Логистическая регрессия, деревья решений | Линейная регрессия, нейронные сети |\n",
    "| Пример | Определение спама | Прогнозирование температуры |\n",
    "\n",
    "### Примеры задач классификации из реальной жизни\n",
    "1. **Медицина** – диагностика болезней на основе симптомов.\n",
    "2. **Финансы** – определение мошеннических транзакций.\n",
    "3. **Интернет-маркетинг** – предсказание кликабельности рекламы.\n",
    "\n",
    "---\n",
    "\n",
    "## 9. Логистическая регрессия\n",
    "\n",
    "### Отличие от линейной регрессии\n",
    "Хотя **логистическая регрессия** называется регрессией, она применяется для **классификации**.\n",
    "\n",
    "- В **линейной регрессии** выходное значение – **непрерывное число**.\n",
    "- В **логистической регрессии** выход – **вероятность принадлежности** к классу.\n",
    "\n",
    "### Формулы и алгоритм построения модели\n",
    "\n",
    "Логистическая регрессия использует **сигмоидную функцию**, чтобы предсказать вероятность класса:\n",
    "\n",
    "$$\n",
    "P(y=1) = \\frac{1}{1 + e^{-(b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n)}}\n",
    "$$\n",
    "\n",
    "Где:\n",
    "- \\\\( b_0, b_1, ..., b_n \\\\) – коэффициенты модели.\n",
    "- \\\\( x_1, x_2, ..., x_n \\\\) – входные признаки.\n",
    "\n",
    "**Как работает алгоритм?**\n",
    "1. Инициализация случайных коэффициентов.\n",
    "2. Вычисление логистической функции.\n",
    "3. Оптимизация параметров с помощью **градиентного спуска**.\n",
    "4. Применение порога (обычно 0.5) для классификации.\n",
    "\n",
    "### Преимущества логистической регрессии\n",
    "✅ **Интерпретируемость** – легко объяснить, какие факторы влияют на результат.  \n",
    "✅ **Быстрота обучения** – быстрее нейронных сетей.  \n",
    "✅ **Работает с вероятностями** – можно определить уверенность предсказания.  \n",
    "\n",
    "### Недостатки логистической регрессии\n",
    "❌ **Не работает с нелинейными зависимостями** – если граница между классами не линейная, модель плохо справляется.  \n",
    "❌ **Чувствительна к выбросам** – аномальные данные могут сильно влиять.  \n",
    "❌ **Плохо работает с большим числом признаков** – модель становится переобученной.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f88a7",
   "metadata": {},
   "source": [
    "## 10. Метрики для задач классификации\n",
    "\n",
    "### Почему важны метрики?\n",
    "При решении задач классификации недостаточно просто измерить **точность (accuracy)**, так как модель может плохо работать на несбалансированных данных. Например, если в наборе данных 95% примеров относятся к классу \"0\" и только 5% – к классу \"1\", то модель, предсказывающая всегда \"0\", будет иметь 95% точности, но фактически не решит задачу.\n",
    "\n",
    "### Основные метрики классификации\n",
    "\n",
    "#### **1. Точность (Accuracy)**\n",
    "- Формула:\n",
    "  $$\n",
    "  Accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "  $$\n",
    "- **Описание**: Доля правильно классифицированных примеров.\n",
    "- **Когда использовать**: Если классы сбалансированы.\n",
    "- **Когда не использовать**: Если классы сильно несбалансированы.\n",
    "\n",
    "#### **2. Полнота (Recall, Sensitivity)**\n",
    "- Формула:\n",
    "  $$\n",
    "  Recall = \\frac{TP}{TP + FN}\n",
    "  $$\n",
    "- **Описание**: Показывает, какая доля объектов **положительного класса** была правильно предсказана.\n",
    "- **Когда важен**: В медицинской диагностике (лучше ошибочно диагностировать болезнь, чем пропустить её).\n",
    "\n",
    "#### **3. Точность (Precision)**\n",
    "- Формула:\n",
    "  $$\n",
    "  Precision = \\frac{TP}{TP + FP}\n",
    "  $$\n",
    "- **Описание**: Показывает, сколько предсказанных **положительных примеров** действительно являются положительными.\n",
    "- **Когда важен**: В системах обнаружения мошенничества (лучше выявить меньше случаев, но с высокой точностью).\n",
    "\n",
    "--\n",
    "\n",
    "#### **4. F1-мера (F1-score)**\n",
    "- Формула:\n",
    "  $$\n",
    "  F1 = 2 \\times \\frac{Precision \\times Recall}{Precision + Recall}\n",
    "  $$\n",
    "- **Описание**: Гармоническое среднее Precision и Recall.\n",
    "- **Когда важен**: Когда важен баланс между полнотой и точностью.\n",
    "\n",
    "#### **5. ROC-кривая и AUC (Area Under Curve)**\n",
    "- **ROC-кривая** – показывает зависимость между **True Positive Rate (Recall)** и **False Positive Rate**.\n",
    "- **AUC (площадь под кривой ROC)** – чем выше AUC, тем лучше модель.\n",
    "- **Когда важен**: Когда необходимо оценить способность модели разделять классы.\n",
    "\n",
    "---\n",
    "\n",
    "## 11. Матрица ошибок (Confusion Matrix)\n",
    "\n",
    "### Определение\n",
    "Матрица ошибок используется для оценки качества модели классификации, показывая количество правильных и ошибочных предсказаний.\n",
    "\n",
    "### Как выглядит матрица ошибок?\n",
    "\n",
    "|   | Предсказано: 1 | Предсказано: 0 |\n",
    "|---|---------------|---------------|\n",
    "| **Фактически 1** | TP (True Positive)  | FN (False Negative)  |\n",
    "| **Фактически 0** | FP (False Positive) | TN (True Negative) |\n",
    "\n",
    "### Расшифровка терминов:\n",
    "- **TP (Истинно положительные)** – модель правильно предсказала положительный класс.\n",
    "- **TN (Истинно отрицательные)** – модель правильно предсказала отрицательный класс.\n",
    "- **FP (Ложно положительные, \"false alarms\")** – модель ошибочно предсказала положительный класс.\n",
    "- **FN (Ложно отрицательные, \"пропущенные случаи\")** – модель ошибочно предсказала отрицательный класс.\n",
    "\n",
    "### Применение в задачах классификации\n",
    "- **Медицина**: Важно минимизировать FN (пропущенные болезни).\n",
    "- **Банковские кредиты**: Важно минимизировать FP (ошибочно одобренные кредиты).\n",
    "- **Антиспам**: Важно минимизировать FN (важные письма, попавшие в спам).\n",
    "\n",
    "---\n",
    "\n",
    "## 12. AUC-ROC\n",
    "\n",
    "### Принципы построения ROC-кривой\n",
    "ROC-кривая (Receiver Operating Characteristic) строится на основе различных пороговых значений вероятности классификатора.\n",
    "\n",
    "1. **True Positive Rate (TPR) = Recall**:\n",
    "   $$\n",
    "   TPR = \\frac{TP}{TP + FN}\n",
    "   $$\n",
    "   Отражает, какая доля **реальных положительных объектов** была правильно классифицирована.\n",
    "\n",
    "2. **False Positive Rate (FPR)**:\n",
    "   $$\n",
    "   FPR = \\frac{FP}{FP + TN}\n",
    "   $$\n",
    "   Показывает, какая доля **реальных отрицательных объектов** была ошибочно предсказана как положительные.\n",
    "\n",
    "### Значение метрики AUC и её особенности\n",
    "- **AUC (Area Under Curve)** – это площадь под ROC-кривой.  \n",
    "- **Интерпретация AUC**:\n",
    "  - **AUC = 1.0** – идеальная модель.\n",
    "  - **AUC = 0.5** – случайные предсказания.\n",
    "  - **AUC < 0.5** – модель хуже случайного угадывания.\n",
    "\n",
    "### Применение для оценки моделей\n",
    "AUC-ROC используется для оценки вероятностных моделей, особенно когда важно учитывать баланс между FP и FN.\n",
    "\n",
    "- **Пример 1: Медицинские тесты**\n",
    "  - Высокий **Recall** (низкий FN) важнее, чем **Precision**.\n",
    "  - AUC-ROC помогает выбрать оптимальный порог.\n",
    "\n",
    "- **Пример 2: Детекторы мошенничества**\n",
    "  - Высокий **Precision** важнее (меньше FP).\n",
    "  - AUC-ROC показывает, насколько модель хорошо отделяет мошенников от обычных пользователей.\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd3e8bd",
   "metadata": {},
   "source": [
    "\n",
    "## 13. Методы валидации\n",
    "\n",
    "### Почему важна валидация?\n",
    "Валидация используется для проверки качества модели на **независимых данных**, чтобы избежать **переобучения** (overfitting) или **недообучения** (underfitting). Она помогает выбрать оптимальные гиперпараметры модели.\n",
    "\n",
    "### Разделение на обучающую и тестовую выборки\n",
    "Один из простейших методов валидации – разделение данных на:\n",
    "- **Обучающую выборку (Training Set)** – используется для подгонки модели.\n",
    "- **Тестовую выборку (Test Set)** – используется для оценки модели.\n",
    "\n",
    "Обычно данные делят в пропорции:\n",
    "- **80% / 20%** (80% – обучение, 20% – тестирование)\n",
    "- **70% / 30%** (70% – обучение, 30% – тестирование)\n",
    "\n",
    "Но при небольшом объеме данных такой метод может давать нестабильные результаты.\n",
    "\n",
    "### Кросс-валидация (Cross-Validation)\n",
    "Используется, когда данных **недостаточно** для надежного тестирования. Позволяет **повторно использовать все данные** для оценки модели.\n",
    "\n",
    "#### **1. K-Fold Cross-Validation**\n",
    "- Данные разбиваются на **K частей (фолдов)**.\n",
    "- **Модель обучается** на \\\\( K-1 \\\\) фолдах, а тестируется на оставшемся.\n",
    "- Этот процесс повторяется \\\\( K \\\\) раз, меняя тестовый фолд.\n",
    "- **Окончательная метрика** – среднее значение по всем итерациям.\n",
    "\n",
    "\n",
    "\n",
    "#### **2. Leave-One-Out (LOO) Cross-Validation**\n",
    "- Частный случай K-Fold, где \\\\( K = N \\\\) (количество наблюдений).\n",
    "- Каждый раз тестируется **ровно один** объект, а все остальные используются для обучения.\n",
    "- Очень точный, но **дорогостоящий** (особенно на больших данных).\n",
    "\n",
    "#### **3. Stratified K-Fold**\n",
    "- Улучшенный вариант K-Fold, **сохраняющий баланс классов** в каждом разбиении.\n",
    "- Полезен для несбалансированных данных.\n",
    "\n",
    "### Итог:\n",
    "- **K-Fold CV** – золотой стандарт для оценки моделей.\n",
    "- **LOO CV** – точный, но слишком затратный.\n",
    "- **Stratified K-Fold** – лучше для дисбалансных данных.\n",
    "\n",
    "---\n",
    "\n",
    "## 14. Недообучение и переобучение\n",
    "\n",
    "### Определение\n",
    "- **Недообучение (Underfitting)** – модель **слишком простая**, не улавливает закономерности в данных.\n",
    "- **Переобучение (Overfitting)** – модель **слишком сложная**, запоминает данные, но не умеет обобщать.\n",
    "\n",
    "### Как обнаружить?\n",
    "\n",
    "#### 1. **Недообучение**\n",
    "**Признаки:**\n",
    "- Высокая ошибка как на **обучающих**, так и на **тестовых данных**.\n",
    "- Модель плохо предсказывает и не находит закономерностей.\n",
    "\n",
    "**Решения:**\n",
    "✅ Добавить больше признаков.  \n",
    "✅ Использовать более сложные модели (нейронные сети, ансамбли).  \n",
    "✅ Уменьшить регуляризацию.  \n",
    "\n",
    "#### 2. **Переобучение**\n",
    "**Признаки:**\n",
    "- Ошибка **очень мала** на обучающей выборке.\n",
    "- Ошибка **значительно выше** на тестовой выборке.\n",
    "\n",
    "**Решения:**\n",
    "✅ Добавить больше данных для обучения.  \n",
    "✅ Упростить модель (уменьшить количество параметров).  \n",
    "✅ Использовать **регуляризацию** (L1, L2).  \n",
    "✅ Использовать кросс-валидацию.  \n",
    "\n",
    "---\n",
    "\n",
    "## 15. Градиентный спуск\n",
    "\n",
    "### Определение\n",
    "Градиентный спуск (Gradient Descent) – это алгоритм **оптимизации**, используемый для минимизации функции потерь модели.\n",
    "\n",
    "### Основная идея\n",
    "1. **Вычислить градиент ошибки** – направление наибольшего роста функции ошибки.\n",
    "2. **Сделать шаг в противоположную сторону** – уменьшить ошибку.\n",
    "3. **Повторять процесс** до достижения минимума функции потерь.\n",
    "\n",
    "### Формула обновления весов:\n",
    "$$\n",
    "w = w - \\alpha \\cdot \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "где:\n",
    "- \\\\( w \\\\) – вес модели.\n",
    "- \\\\( \\alpha \\\\) – **скорость обучения** (learning rate).\n",
    "- \\\\( \\frac{\\partial L}{\\partial w} \\\\) – градиент функции потерь \\\\( L \\\\).\n",
    "\n",
    "### Типы градиентного спуска:\n",
    "\n",
    "#### **1. Обычный (Batch Gradient Descent)**\n",
    "- Вычисляет градиент **на всех данных сразу**.\n",
    "- Точнее, но **медленный** при больших данных.\n",
    "\n",
    "#### **2. Стохастический (SGD – Stochastic Gradient Descent)**\n",
    "- Обновляет параметры **после каждого отдельного примера**.\n",
    "- Быстрый, но **шумный** (не всегда сходится к оптимальному решению).\n",
    "\n",
    "#### **3. Мини-пакетный градиентный спуск (Mini-Batch Gradient Descent)**\n",
    "- Делит данные на **небольшие группы (батчи)**.\n",
    "- Компромисс между Batch и SGD – **быстрее, но стабильнее**.\n",
    "\n",
    "### Скорость обучения (learning rate)\n",
    "- **Если \\\\( \\alpha \\\\) слишком маленький** – обучение долгое.\n",
    "- **Если \\\\( \\alpha \\\\) слишком большой** – модель может не сойтись.\n",
    "\n",
    "**Решение**:  \n",
    "Использовать **адаптивные алгоритмы**:\n",
    "- **Adam** – автоматически подбирает скорость обучения.\n",
    "- **RMSprop** – стабилизирует обучение.\n",
    "\n",
    "### Итог:\n",
    "- Градиентный спуск – **ключевой алгоритм оптимизации**.\n",
    "- **Mini-Batch Gradient Descent** – лучший компромисс между скоростью и точностью.\n",
    "- **Adam** – улучшенный алгоритм для более быстрой сходимости.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1addc39e",
   "metadata": {},
   "source": [
    "# Дополнительно\n",
    "\n",
    "## Полная формула **коэффициента детерминации \\\\( R^2 \\\\)**, который используется для оценки качества модели регрессии:\n",
    "\n",
    "\\\\[\n",
    "R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n",
    "\\\\]\n",
    "\n",
    "где:\n",
    "\n",
    "- \\\\( SS_{res} \\\\) – сумма квадратов остатков (ошибок предсказаний):\n",
    "  \\\\[\n",
    "  SS_{res} = \\sum_{i=1}^{n} (y_i - \\hat{y_i})^2\n",
    "  \\\\]\n",
    "  где \\\\( y_i \\\\) – реальные значения, \\\\( \\hat{y_i} \\\\) – предсказанные значения.\n",
    "\n",
    "- \\\\( SS_{tot} \\\\) – полная сумма квадратов (дисперсия реальных значений):\n",
    "  \\\\[\n",
    "  SS_{tot} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
    "  \\\\]\n",
    "  где \\\\( \\bar{y} \\\\) – среднее значение целевой переменной:\n",
    "  \\\\[\n",
    "  \\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i\n",
    "  \\\\]\n",
    "\n",
    "### **Расшифровка формулы**\n",
    "- \\\\( SS_{tot} \\\\) показывает **разброс фактических данных**.\n",
    "- \\\\( SS_{res} \\\\) показывает **разброс ошибок предсказания**.\n",
    "- Если \\\\( R^2 \\\\) **близок к 1**, модель хорошо объясняет данные.\n",
    "- Если \\\\( R^2 \\\\) **близок к 0**, модель плохо предсказывает.\n",
    "- Если \\\\( R^2 \\\\) **отрицателен**, модель хуже, чем простое среднее.\n",
    "\n",
    "### **Пример расчета**\n",
    "Допустим, есть реальные значения:  \n",
    "\\\\( y = [3, 5, 7, 9] \\\\)  \n",
    "Предсказанные значения:  \n",
    "\\\\( \\hat{y} = [2.5, 5.3, 6.8, 8.9] \\\\)  \n",
    "\n",
    "1. **Найдем \\\\( \\bar{y} \\\\):**\n",
    "   \\\\[\n",
    "   \\bar{y} = \\frac{3 + 5 + 7 + 9}{4} = 6\n",
    "   \\\\]\n",
    "\n",
    "2. **Вычислим \\\\( SS_{tot} \\\\):**\n",
    "   \\\\[\n",
    "   SS_{tot} = (3 - 6)^2 + (5 - 6)^2 + (7 - 6)^2 + (9 - 6)^2\n",
    "   \\\\]\n",
    "   \\\\[\n",
    "   = 9 + 1 + 1 + 9 = 20\n",
    "   \\\\]\n",
    "\n",
    "3. **Вычислим \\\\( SS_{res} \\\\):**\n",
    "   \\\\[\n",
    "   SS_{res} = (3 - 2.5)^2 + (5 - 5.3)^2 + (7 - 6.8)^2 + (9 - 8.9)^2\n",
    "   \\\\]\n",
    "   \\\\[\n",
    "   = 0.25 + 0.09 + 0.04 + 0.01 = 0.39\n",
    "   \\\\]\n",
    "\n",
    "4. **Вычислим \\\\( R^2 \\\\):**\n",
    "   \\\\[\n",
    "   R^2 = 1 - \\frac{0.39}{20} = 1 - 0.0195 = 0.9805\n",
    "   \\\\]\n",
    "\n",
    "**Вывод:** \\\\( R^2 = 0.98 \\\\) – модель **очень хорошо** предсказывает данные!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecb22fd",
   "metadata": {},
   "source": [
    "# **Полная формула логистической регрессии**\n",
    "\n",
    "Логистическая регрессия используется для решения задач бинарной классификации, где целевая переменная \\\\( y \\\\) принимает значения **0 или 1**.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Основное уравнение логистической регрессии**\n",
    "Логистическая регрессия предсказывает **вероятность принадлежности объекта к классу 1** с помощью **сигмоидной функции**:\n",
    "\n",
    "\\\\[\n",
    "P(y = 1 \\mid X) = \\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
    "\\\\]\n",
    "\n",
    "где:\n",
    "- \\\\( z \\\\) – **линейная комбинация входных признаков**:\n",
    "  \\\\[\n",
    "  z = b_0 + b_1 x_1 + b_2 x_2 + ... + b_n x_n\n",
    "  \\\\]\n",
    "- \\\\( \\sigma(z) \\\\) – **сигмоидная функция** (она сжимает значения в диапазон от 0 до 1).\n",
    "- \\\\( b_0, b_1, ..., b_n \\\\) – **коэффициенты модели** (веса).\n",
    "- \\\\( x_1, x_2, ..., x_n \\\\) – **входные признаки**.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Функция потерь логистической регрессии**\n",
    "Чтобы найти **оптимальные коэффициенты** \\\\( b \\\\), используется **логарифмическая функция правдоподобия** (Log-Likelihood):\n",
    "\n",
    "\\\\[\n",
    "L(b) = \\sum_{i=1}^{n} \\left[ y_i \\log P(y_i) + (1 - y_i) \\log (1 - P(y_i)) \\right]\n",
    "\\\\]\n",
    "\n",
    "Так как мы **максимизируем правдоподобие**, для удобства используется **негативная логарифмическая функция потерь (Log Loss)**:\n",
    "\n",
    "\\\\[\n",
    "J(b) = - \\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log P(y_i) + (1 - y_i) \\log (1 - P(y_i)) \\right]\n",
    "\\\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Градиентный спуск для обновления весов**\n",
    "Градиентный спуск используется для **оптимизации весов** \\\\( b \\\\):\n",
    "\n",
    "\\\\[\n",
    "b_j = b_j - \\alpha \\cdot \\frac{\\partial J(b)}{\\partial b_j}\n",
    "\\\\]\n",
    "\n",
    "Где:\n",
    "- \\\\( \\alpha \\\\) – **скорость обучения (learning rate)**.\n",
    "- Частная производная по весам \\\\( b_j \\\\):\n",
    "\n",
    "\\\\[\n",
    "\\frac{\\partial J(b)}{\\partial b_j} = \\frac{1}{n} \\sum_{i=1}^{n} (P(y_i) - y_i) x_{ij}\n",
    "\\\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Разделяющая гиперплоскость**\n",
    "Решающее правило в логистической регрессии:\n",
    "\n",
    "\\\\[\n",
    "y = \n",
    "\\begin{cases} \n",
    "1, & \\text{если } P(y=1) \\geq 0.5 \\\\\n",
    "0, & \\text{если } P(y=1) < 0.5\n",
    "\\end{cases}\n",
    "\\\\]\n",
    "\n",
    "Это означает, что граница раздела проходит там, где:\n",
    "\n",
    "\\\\[\n",
    "b_0 + b_1 x_1 + b_2 x_2 + ... + b_n x_n = 0\n",
    "\\\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Пример численного расчета**\n",
    "Допустим, у нас есть две переменные:\n",
    "\n",
    "| \\\\( x_1 \\\\) (возраст) | \\\\( x_2 \\\\) (доход) | \\\\( y \\\\) (купил товар) |\n",
    "|----------------|------------|---------------|\n",
    "| 25           | 30 000     | 0             |\n",
    "| 35           | 50 000     | 1             |\n",
    "| 45           | 70 000     | 1             |\n",
    "\n",
    "Модель имеет коэффициенты:\n",
    "- \\\\( b_0 = -10 \\\\)\n",
    "- \\\\( b_1 = 0.05 \\\\)\n",
    "- \\\\( b_2 = 0.0001 \\\\)\n",
    "\n",
    "Для человека **35 лет с доходом 50 000**:\n",
    "\n",
    "1. **Вычисляем линейную комбинацию**:\n",
    "   \\\\[\n",
    "   z = -10 + (0.05 \\times 35) + (0.0001 \\times 50000)\n",
    "   \\\\]\n",
    "   \\\\[\n",
    "   z = -10 + 1.75 + 5 = -3.25\n",
    "   \\\\]\n",
    "\n",
    "2. **Применяем сигмоид**:\n",
    "   \\\\[\n",
    "   P(y = 1) = \\frac{1}{1 + e^{-(-3.25)}}\n",
    "   \\\\]\n",
    "   \\\\[\n",
    "   = \\frac{1}{1 + e^{3.25}} = \\frac{1}{1 + 25.8} \\approx 0.037\n",
    "   \\\\]\n",
    "\n",
    "3. **Классифицируем**:\n",
    "   - \\\\( P(y=1) = 0.037 \\\\) (меньше 0.5) → предсказанный класс **0** (не купил товар).\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Итог**\n",
    "Логистическая регрессия – это мощный алгоритм бинарной классификации, который:\n",
    "✅ Выдает вероятность класса.  \n",
    "✅ Легко интерпретируется.  \n",
    "✅ Использует логистическую функцию и градиентный спуск для оптимизации.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e1e06",
   "metadata": {},
   "source": [
    "# **Полные формулы нормализации данных**\n",
    "\n",
    "Нормализация данных важна в машинном обучении, так как многие алгоритмы чувствительны к масштабу входных признаков. Основные методы нормализации:\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Min-Max Scaling (Нормализация Min-Max)**\n",
    "Этот метод **масштабирует значения** признака в диапазон **[0, 1]** или **[-1, 1]**.\n",
    "\n",
    "### **Формула:**\n",
    "\\\\[\n",
    "X_{\\text{norm}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
    "\\\\]\n",
    "\n",
    "### **Для диапазона [-1, 1]:**\n",
    "\\\\[\n",
    "X_{\\text{norm}} = 2 \\times \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}} - 1\n",
    "\\\\]\n",
    "\n",
    "### **Расшифровка:**\n",
    "- \\\\( X \\\\) – исходное значение признака.\n",
    "- \\\\( X_{\\min} \\\\) – минимальное значение признака в выборке.\n",
    "- \\\\( X_{\\max} \\\\) – максимальное значение признака в выборке.\n",
    "- \\\\( X_{\\text{norm}} \\\\) – нормализованное значение.\n",
    "\n",
    "### **Пример расчета**\n",
    "Пусть у нас есть признак \"Возраст\" с диапазоном **[20, 60]**. Для \\\\( X = 40 \\\\):\n",
    "\n",
    "\\\\[\n",
    "X_{\\text{norm}} = \\frac{40 - 20}{60 - 20} = \\frac{20}{40} = 0.5\n",
    "\\\\]\n",
    "\n",
    "Если диапазон **[-1, 1]**:\n",
    "\n",
    "\\\\[\n",
    "X_{\\text{norm}} = 2 \\times 0.5 - 1 = 0\n",
    "\\\\]\n",
    "\n",
    "✅ **Когда использовать:**  \n",
    "- Когда признаки имеют **разные масштабы**.  \n",
    "- Когда важны относительные различия значений.\n",
    "\n",
    "❌ **Недостатки:**  \n",
    "- Чувствителен к выбросам: если \\\\( X_{\\max} \\\\) или \\\\( X_{\\min} \\\\) – выброс, вся нормализация нарушается.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Z-score Scaling (Стандартизация)**\n",
    "Этот метод **центрирует данные** вокруг **0** и делает стандартное отклонение **1**.\n",
    "\n",
    "### **Формула:**\n",
    "\\\\[\n",
    "X_{\\text{std}} = \\frac{X - \\mu}{\\sigma}\n",
    "\\\\]\n",
    "\n",
    "### **Расшифровка:**\n",
    "- \\\\( X \\\\) – исходное значение.\n",
    "- \\\\( \\mu \\\\) – среднее значение признака.\n",
    "- \\\\( \\sigma \\\\) – стандартное отклонение признака.\n",
    "- \\\\( X_{\\text{std}} \\\\) – стандартизированное значение.\n",
    "\n",
    "### **Пример расчета**\n",
    "Допустим, средний возраст в выборке **\\\\( \\mu = 40 \\\\)**, стандартное отклонение **\\\\( \\sigma = 10 \\\\)**, и у нас есть значение **\\\\( X = 50 \\\\)**:\n",
    "\n",
    "\\\\[\n",
    "X_{\\text{std}} = \\frac{50 - 40}{10} = \\frac{10}{10} = 1\n",
    "\\\\]\n",
    "\n",
    "✅ **Когда использовать:**  \n",
    "- Когда данные имеют **разное распределение**.  \n",
    "- Когда алгоритмы чувствительны к дисбалансу (градиентный спуск, PCA, KNN).\n",
    "\n",
    "❌ **Недостатки:**  \n",
    "- Если распределение не **нормальное (Gaussian)**, метод может работать плохо.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Mean Normalization (Нормализация относительно среднего)**\n",
    "Похожа на Z-score, но нормализует диапазон значений **в пределах [-1, 1]**.\n",
    "\n",
    "### **Формула:**\n",
    "\\\\[\n",
    "X_{\\text{mean-norm}} = \\frac{X - \\mu}{X_{\\max} - X_{\\min}}\n",
    "\\\\]\n",
    "\n",
    "### **Пример расчета**\n",
    "Пусть **\\\\( X_{\\max} = 60 \\\\)**, **\\\\( X_{\\min} = 20 \\\\)**, **\\\\( \\mu = 40 \\\\)**, и **\\\\( X = 50 \\\\)**:\n",
    "\n",
    "\\\\[\n",
    "X_{\\text{mean-norm}} = \\frac{50 - 40}{60 - 20} = \\frac{10}{40} = 0.25\n",
    "\\\\]\n",
    "\n",
    "✅ **Когда использовать:**  \n",
    "- Когда важны **относительные масштабы**, но данные не распределены нормально.\n",
    "\n",
    "❌ **Недостатки:**  \n",
    "- Теряет информацию о стандартном отклонении.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Robust Scaling (Масштабирование на основе квантилей)**\n",
    "Используется, если в данных есть **выбросы**.\n",
    "\n",
    "### **Формула:**\n",
    "\\\\[\n",
    "X_{\\text{robust}} = \\frac{X - Q_2}{Q_3 - Q_1}\n",
    "\\\\]\n",
    "\n",
    "### **Расшифровка:**\n",
    "- \\\\( Q_1 \\\\) – 25-й процентиль (первая квартиль).\n",
    "- \\\\( Q_2 \\\\) – медиана (50-й процентиль).\n",
    "- \\\\( Q_3 \\\\) – 75-й процентиль (третья квартиль).\n",
    "\n",
    "### **Пример расчета**\n",
    "Пусть у нас:\n",
    "- **\\\\( Q_1 = 30 \\\\)**,\n",
    "- **\\\\( Q_2 = 50 \\\\)**,\n",
    "- **\\\\( Q_3 = 70 \\\\)**,\n",
    "- и \\\\( X = 60 \\\\).\n",
    "\n",
    "Тогда:\n",
    "\n",
    "\\\\[\n",
    "X_{\\text{robust}} = \\frac{60 - 50}{70 - 30} = \\frac{10}{40} = 0.25\n",
    "\\\\]\n",
    "\n",
    "✅ **Когда использовать:**  \n",
    "- Когда в данных есть выбросы.\n",
    "\n",
    "❌ **Недостатки:**  \n",
    "- Не работает, если распределение **очень скошенное**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Какой метод выбрать?**\n",
    "| Метод | Подходит для | Чувствителен к выбросам? | Диапазон |\n",
    "|-------|------------|-----------------|---------|\n",
    "| **Min-Max Scaling** | Глубокое обучение, KNN, SVM | ✅ Да | [0,1] или [-1,1] |\n",
    "| **Z-score (Standardization)** | Градиентный спуск, PCA, линейная регрессия | ❌ Нет | Среднее = 0, σ = 1 |\n",
    "| **Mean Normalization** | Глубокое обучение | ✅ Да | [-1,1] |\n",
    "| **Robust Scaling** | Данные с выбросами | ❌ Нет | Зависит от квартилей |\n",
    "\n",
    "---\n",
    "\n",
    "## **Заключение**\n",
    "1. Если **нет выбросов**, лучше использовать **Min-Max Scaling**.\n",
    "2. Если **данные нормально распределены**, лучше использовать **Z-score**.\n",
    "3. Если **есть выбросы**, **Robust Scaling** – лучший вариант.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c8f5b1",
   "metadata": {},
   "source": [
    "### **Полная формула стандартизации (Standard Scaler, Z-score Scaling)**\n",
    "\n",
    "**Стандартизация (Z-score Scaling)** – это метод нормализации, при котором данные центрируются вокруг **0** и приводятся к стандартному отклонению **1**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Полная формула стандартизации:**\n",
    "\\\\[\n",
    "X_{\\text{std}} = \\frac{X - \\mu}{\\sigma}\n",
    "\\\\]\n",
    "\n",
    "где:\n",
    "- \\\\( X_{\\text{std}} \\\\) – стандартизированное значение.\n",
    "- \\\\( X \\\\) – исходное значение признака.\n",
    "- \\\\( \\mu \\\\) – **среднее значение признака**:\n",
    "  \\\\[\n",
    "  \\mu = \\frac{1}{n} \\sum_{i=1}^{n} X_i\n",
    "  \\\\]\n",
    "- \\\\( \\sigma \\\\) – **стандартное отклонение признака**:\n",
    "  \\\\[\n",
    "  \\sigma = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (X_i - \\mu)^2}\n",
    "  \\\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Раскрытие формулы по шагам**\n",
    "1. **Найти среднее значение (\\\\( \\mu \\\\))**:\n",
    "   \\\\[\n",
    "   \\mu = \\frac{1}{n} \\sum X_i\n",
    "   \\\\]\n",
    "   (сумма всех значений признака делится на количество объектов).\n",
    "\n",
    "2. **Найти стандартное отклонение (\\\\( \\sigma \\\\))**:\n",
    "   \\\\[\n",
    "   \\sigma = \\sqrt{\\frac{1}{n} \\sum (X_i - \\mu)^2}\n",
    "   \\\\]\n",
    "   (сначала вычисляется дисперсия, затем берется корень).\n",
    "\n",
    "3. **Вычислить стандартизированное значение**:\n",
    "   \\\\[\n",
    "   X_{\\text{std}} = \\frac{X - \\mu}{\\sigma}\n",
    "   \\\\]\n",
    "   (вычитаем среднее значение и делим на стандартное отклонение).\n",
    "\n",
    "---\n",
    "\n",
    "### **Пример расчета**\n",
    "Допустим, у нас есть выборка **\\\\( X = [10, 20, 30, 40, 50] \\\\)**.\n",
    "\n",
    "1. **Среднее значение**:\n",
    "   \\\\[\n",
    "   \\mu = \\frac{10 + 20 + 30 + 40 + 50}{5} = 30\n",
    "   \\\\]\n",
    "\n",
    "2. **Стандартное отклонение**:\n",
    "   - Вычислим сумму квадратов отклонений:\n",
    "     \\\\[\n",
    "     \\sum (X_i - \\mu)^2 = (10 - 30)^2 + (20 - 30)^2 + (30 - 30)^2 + (40 - 30)^2 + (50 - 30)^2\n",
    "     \\\\]\n",
    "     \\\\[\n",
    "     = (-20)^2 + (-10)^2 + (0)^2 + (10)^2 + (20)^2 = 400 + 100 + 0 + 100 + 400 = 1000\n",
    "     \\\\]\n",
    "   - Разделим на количество элементов:\n",
    "     \\\\[\n",
    "     \\sigma = \\sqrt{\\frac{1000}{5}} = \\sqrt{200} \\approx 14.14\n",
    "     \\\\]\n",
    "\n",
    "3. **Стандартизация каждого значения**:\n",
    "   - \\\\( X = 10 \\\\):  \n",
    "     \\\\[\n",
    "     X_{\\text{std}} = \\frac{10 - 30}{14.14} = \\frac{-20}{14.14} \\approx -1.41\n",
    "     \\\\]\n",
    "   - \\\\( X = 20 \\\\):  \n",
    "     \\\\[\n",
    "     X_{\\text{std}} = \\frac{20 - 30}{14.14} = \\frac{-10}{14.14} \\approx -0.71\n",
    "     \\\\]\n",
    "   - \\\\( X = 30 \\\\):  \n",
    "     \\\\[\n",
    "     X_{\\text{std}} = \\frac{30 - 30}{14.14} = 0\n",
    "     \\\\]\n",
    "   - \\\\( X = 40 \\\\):  \n",
    "     \\\\[\n",
    "     X_{\\text{std}} = \\frac{40 - 30}{14.14} = \\frac{10}{14.14} \\approx 0.71\n",
    "     \\\\]\n",
    "   - \\\\( X = 50 \\\\):  \n",
    "     \\\\[\n",
    "     X_{\\text{std}} = \\frac{50 - 30}{14.14} = \\frac{20}{14.14} \\approx 1.41\n",
    "     \\\\]\n",
    "\n",
    "📌 **Выходные значения после стандартизации:**  \n",
    "\\\\( X_{\\text{std}} \\approx [-1.41, -0.71, 0, 0.71, 1.41] \\\\).\n",
    "\n",
    "---\n",
    "\n",
    "### **Свойства стандартизации**\n",
    "✅ **Среднее после стандартизации всегда 0**  \n",
    "✅ **Стандартное отклонение после стандартизации всегда 1**  \n",
    "✅ **Работает даже при наличии выбросов, но может быть чувствителен к ним**  \n",
    "\n",
    "---\n",
    "\n",
    "### **Когда использовать Standard Scaler?**\n",
    "🔹 **Подходит для**:  \n",
    "- Линейной регрессии  \n",
    "- Логистической регрессии  \n",
    "- PCA (методов понижения размерности)  \n",
    "- KNN, SVM, градиентного спуска  \n",
    "\n",
    "🔹 **Не подходит для**:  \n",
    "- Данных с выбросами (лучше использовать **Robust Scaler**)  \n",
    "- Когда нужен фиксированный диапазон (лучше **Min-Max Scaling**)  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55214e9b",
   "metadata": {},
   "source": [
    "# **L1 и L2-регуляризация: Подробное объяснение**\n",
    "\n",
    "## **Что такое регуляризация?**\n",
    "Регуляризация – это метод, используемый в **машинном обучении** для **предотвращения переобучения** (overfitting). Она добавляет штраф к функции потерь, чтобы модель **не запоминала данные**, а обобщала закономерности.\n",
    "\n",
    "---\n",
    "\n",
    "# **L1-регуляризация (Lasso, Least Absolute Shrinkage and Selection Operator)**\n",
    "\n",
    "### **Полная формула L1-регуляризации**\n",
    "L1-регуляризация добавляет к функции потерь **сумму модулей весов**:\n",
    "\n",
    "\\\\[\n",
    "J(w) = L + \\lambda \\sum_{j=1}^{n} |w_j|\n",
    "\\\\]\n",
    "\n",
    "Где:\n",
    "- \\\\( J(w) \\\\) – **регуляризованная функция потерь**.\n",
    "- \\\\( L \\\\) – **исходная функция потерь** (например, MSE или логистическая).\n",
    "- \\\\( \\lambda \\\\) – **гиперпараметр**, который определяет, насколько сильное влияние оказывает регуляризация.\n",
    "- \\\\( \\sum |w_j| \\\\) – **сумма абсолютных значений весов модели**.\n",
    "\n",
    "### **Как L1-регуляризация влияет на модель?**\n",
    "1. **Зануляет некоторые коэффициенты** (\\\\( w_j = 0 \\\\)) – используется для **отбора признаков**.\n",
    "2. **Создает разреженные модели** – подходит для ситуаций, когда **много признаков, но не все важны**.\n",
    "3. **Может полностью исключать неважные признаки**.\n",
    "\n",
    "### **Пример L1-регуляризации**\n",
    "Если у нас есть линейная модель:\n",
    "\n",
    "\\\\[\n",
    "y = w_1 x_1 + w_2 x_2 + w_3 x_3 + b\n",
    "\\\\]\n",
    "\n",
    "Допустим, веса:\n",
    "- \\\\( w_1 = 2.5 \\\\)\n",
    "- \\\\( w_2 = 0.1 \\\\)\n",
    "- \\\\( w_3 = 0.0001 \\\\)\n",
    "\n",
    "При **L1-регуляризации** модель может занулить \\\\( w_3 \\\\), если он не важен:\n",
    "\n",
    "\\\\[\n",
    "y = 2.5x_1 + 0.1x_2 + 0\n",
    "\\\\]\n",
    "\n",
    "✅ **Используется, если нужно отбирать важные признаки.**\n",
    "\n",
    "❌ **Не идеальна, если признаки сильно коррелированы.**\n",
    "\n",
    "---\n",
    "\n",
    "# **L2-регуляризация (Ridge-регрессия)**\n",
    "\n",
    "### **Полная формула L2-регуляризации**\n",
    "L2-регуляризация добавляет **сумму квадратов весов**:\n",
    "\n",
    "\\\\[\n",
    "J(w) = L + \\lambda \\sum_{j=1}^{n} w_j^2\n",
    "\\\\]\n",
    "\n",
    "Где:\n",
    "- \\\\( \\sum w_j^2 \\\\) – сумма квадратов весов (чем больше веса, тем больше штраф).\n",
    "- Остальные обозначения такие же, как в L1-регуляризации.\n",
    "\n",
    "### **Как L2-регуляризация влияет на модель?**\n",
    "1. **Не зануляет коэффициенты**, а **уменьшает их**.\n",
    "2. **Гладит веса** – модель становится более **стабильной**.\n",
    "3. **Подходит для случаев, когда все признаки важны, но их влияние нужно сгладить**.\n",
    "\n",
    "### **Пример L2-регуляризации**\n",
    "Допустим, начальные веса:\n",
    "\n",
    "- \\\\( w_1 = 10 \\\\)\n",
    "- \\\\( w_2 = 5 \\\\)\n",
    "- \\\\( w_3 = 0.5 \\\\)\n",
    "\n",
    "После **L2-регуляризации** они станут:\n",
    "\n",
    "- \\\\( w_1 = 4.5 \\\\)\n",
    "- \\\\( w_2 = 2.8 \\\\)\n",
    "- \\\\( w_3 = 0.3 \\\\)\n",
    "\n",
    "✅ **Подходит, если признаки сильно коррелированы.**  \n",
    "❌ **Не отбрасывает ненужные признаки.**\n",
    "\n",
    "---\n",
    "\n",
    "# **Сравнение L1 vs L2-регуляризации**\n",
    "\n",
    "| Характеристика  | L1-регуляризация (Lasso) | L2-регуляризация (Ridge) |\n",
    "|----------------|----------------------|----------------------|\n",
    "| **Формула штрафа**  | \\\\( \\sum |w_j| \\\\)  | \\\\( \\sum w_j^2 \\\\)  |\n",
    "| **Эффект на веса**  | Обнуляет некоторые веса  | Уменьшает все веса  |\n",
    "| **Применение**  | Если важен **отбор признаков**  | Если важна **стабильность модели**  |\n",
    "| **Использование**  | Логистическая регрессия, Lasso-регрессия  | Линейная регрессия, Ridge-регрессия  |\n",
    "\n",
    "---\n",
    "\n",
    "# **Elastic Net – комбинация L1 и L2**\n",
    "Если **оба метода важны**, можно использовать **Elastic Net**, который объединяет L1 и L2:\n",
    "\n",
    "\\\\[\n",
    "J(w) = L + \\lambda_1 \\sum |w_j| + \\lambda_2 \\sum w_j^2\n",
    "\\\\]\n",
    "\n",
    "Где:\n",
    "- \\\\( \\lambda_1 \\\\) – вес L1-регуляризации.\n",
    "- \\\\( \\lambda_2 \\\\) – вес L2-регуляризации.\n",
    "\n",
    "### **Когда использовать Elastic Net?**\n",
    "- Когда **много признаков**, но некоторые не важны.\n",
    "- Когда признаки **сильно коррелированы**.\n",
    "\n",
    "---\n",
    "\n",
    "### **Какой метод выбрать?**\n",
    "| Когда?  | Лучший метод |\n",
    "|---------|-------------|\n",
    "| **Много неважных признаков**  | L1 (Lasso) |\n",
    "| **Все признаки важны, но веса слишком большие**  | L2 (Ridge) |\n",
    "| **Много признаков + корреляция**  | Elastic Net |\n",
    "\n",
    "---\n",
    "\n",
    "## **Заключение**\n",
    "🔹 **L1-регуляризация** → отбрасывает неважные признаки.  \n",
    "🔹 **L2-регуляризация** → уменьшает все веса, делая модель устойчивее.  \n",
    "🔹 **Elastic Net** → сочетает преимущества обоих методов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc89dd3",
   "metadata": {},
   "source": [
    "Вот полный разбор **L1**, **L2** и **Elastic Net** с **формулами, объяснениями и примерами кода на Python**.\n",
    "\n",
    "---\n",
    "\n",
    "# **L1 и L2-регуляризация: Полное объяснение**\n",
    "\n",
    "Регуляризация – это метод, используемый для **уменьшения переобучения** и **стабилизации модели**.  \n",
    "Она добавляет **штраф** к функции потерь, который ограничивает рост коэффициентов модели.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. L1-регуляризация (Lasso)**\n",
    "L1-регуляризация добавляет **сумму абсолютных значений весов** в функцию потерь.\n",
    "\n",
    "### **Формула L1-регуляризации (Lasso)**\n",
    "\\\\[\n",
    "J(w) = L + \\lambda \\sum_{j=1}^{n} |w_j|\n",
    "\\\\]\n",
    "Где:\n",
    "- \\\\( J(w) \\\\) – **регуляризованная функция потерь**.\n",
    "- \\\\( L \\\\) – **исходная функция потерь** (например, MSE в линейной регрессии).\n",
    "- \\\\( \\lambda \\\\) – **коэффициент регуляризации** (чем больше, тем сильнее влияние регуляризации).\n",
    "- \\\\( \\sum |w_j| \\\\) – **сумма абсолютных значений весов модели**.\n",
    "\n",
    "### **Как влияет L1-регуляризация?**\n",
    "✅ **Обнуляет ненужные веса** – подходит для **отбора признаков**.  \n",
    "✅ **Создает разреженные модели** – может сделать веса равными **нулю**.  \n",
    "❌ **Плохо работает, если признаки коррелированы**.\n",
    "\n",
    "### **Пример L1-регуляризации**\n",
    "Допустим, у нас есть линейная модель:\n",
    "\n",
    "\\\\[\n",
    "y = w_1 x_1 + w_2 x_2 + w_3 x_3 + b\n",
    "\\\\]\n",
    "\n",
    "Если после применения L1-регуляризации \\\\( w_3 = 0 \\\\), модель упростится:\n",
    "\n",
    "\\\\[\n",
    "y = w_1 x_1 + w_2 x_2 + 0\n",
    "\\\\]\n",
    "\n",
    "---\n",
    "\n",
    "## **2. L2-регуляризация (Ridge)**\n",
    "L2-регуляризация добавляет **сумму квадратов весов** в функцию потерь.\n",
    "\n",
    "### **Формула L2-регуляризации (Ridge)**\n",
    "\\\\[\n",
    "J(w) = L + \\lambda \\sum_{j=1}^{n} w_j^2\n",
    "\\\\]\n",
    "\n",
    "Где:\n",
    "- \\\\( \\sum w_j^2 \\\\) – сумма квадратов весов (чем больше веса, тем больше штраф).\n",
    "\n",
    "### **Как влияет L2-регуляризация?**\n",
    "✅ **Не зануляет коэффициенты**, а **уменьшает их**.  \n",
    "✅ **Гладит веса**, делая модель **стабильной**.  \n",
    "✅ **Работает лучше, если признаки коррелированы**.  \n",
    "❌ **Не отбрасывает ненужные признаки**.\n",
    "\n",
    "### **Пример L2-регуляризации**\n",
    "Допустим, начальные веса:\n",
    "- \\\\( w_1 = 10 \\\\)\n",
    "- \\\\( w_2 = 5 \\\\)\n",
    "- \\\\( w_3 = 0.5 \\\\)\n",
    "\n",
    "После **L2-регуляризации**:\n",
    "- \\\\( w_1 = 4.5 \\\\)\n",
    "- \\\\( w_2 = 2.8 \\\\)\n",
    "- \\\\( w_3 = 0.3 \\\\)\n",
    "\n",
    "Все коэффициенты уменьшились, но не занулились.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Elastic Net (L1 + L2)**\n",
    "**Elastic Net** – это **комбинация L1 и L2**, которая **обнуляет неважные признаки и сглаживает важные**.\n",
    "\n",
    "### **Формула Elastic Net**\n",
    "\\\\[\n",
    "J(w) = L + \\lambda_1 \\sum |w_j| + \\lambda_2 \\sum w_j^2\n",
    "\\\\]\n",
    "\n",
    "Где:\n",
    "- \\\\( \\lambda_1 \\\\) – вес L1-регуляризации.\n",
    "- \\\\( \\lambda_2 \\\\) – вес L2-регуляризации.\n",
    "\n",
    "### **Когда использовать Elastic Net?**\n",
    "✅ Если **есть много признаков**, но **некоторые из них неважны**.  \n",
    "✅ Если **признаки сильно коррелированы**.\n",
    "\n",
    "---\n",
    "\n",
    "# **4. Python: Реализация L1, L2 и Elastic Net**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa555725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso (L1) Коэффициенты: [ 1.60523909 -1.18161317  0.          0.         -0.        ]\n",
      "MSE Lasso: 1.3184547400277624\n",
      "Ridge (L2) Коэффициенты: [ 2.61735183 -2.20590232  0.6306584   0.14617229 -0.4404581 ]\n",
      "MSE Ridge: 0.9826748875590386\n",
      "Elastic Net Коэффициенты: [ 1.39450277 -1.13171754  0.          0.         -0.        ]\n",
      "MSE Elastic Net: 1.4033293450329434\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Генерация данных\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 5)  # 5 признаков\n",
    "y = 3 * X[:, 0] - 2 * X[:, 1] + np.random.randn(100)  # Истинная зависимость\n",
    "\n",
    "# Разделение на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# --- L1-регуляризация (Lasso) ---\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "\n",
    "print(\"Lasso (L1) Коэффициенты:\", lasso.coef_)\n",
    "print(\"MSE Lasso:\", mse_lasso)\n",
    "\n",
    "# --- L2-регуляризация (Ridge) ---\n",
    "ridge = Ridge(alpha=0.1)\n",
    "ridge.fit(X_train, y_train)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "\n",
    "print(\"Ridge (L2) Коэффициенты:\", ridge.coef_)\n",
    "print(\"MSE Ridge:\", mse_ridge)\n",
    "\n",
    "# --- Elastic Net ---\n",
    "elastic_net = ElasticNet(alpha=0.1, l1_ratio=0.5)  # 50% L1, 50% L2\n",
    "elastic_net.fit(X_train, y_train)\n",
    "y_pred_elastic = elastic_net.predict(X_test)\n",
    "mse_elastic = mean_squared_error(y_test, y_pred_elastic)\n",
    "\n",
    "print(\"Elastic Net Коэффициенты:\", elastic_net.coef_)\n",
    "print(\"MSE Elastic Net:\", mse_elastic)\n",
    "\n",
    "# Визуализация коэффициентов\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(lasso.coef_)), lasso.coef_, label=\"Lasso (L1)\", alpha=0.6)\n",
    "plt.bar(range(len(ridge.coef_)), ridge.coef_, label=\"Ridge (L2)\", alpha=0.6)\n",
    "plt.bar(range(len(elastic_net.coef_)), elastic_net.coef_, label=\"Elastic Net\", alpha=0.6)\n",
    "plt.xlabel(\"Признаки\")\n",
    "plt.ylabel(\"Коэффициенты\")\n",
    "plt.legend()\n",
    "plt.title(\"Сравнение L1, L2 и Elastic Net\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9342f4",
   "metadata": {},
   "source": [
    "## **5. Сравнение L1, L2 и Elastic Net**\n",
    "\n",
    "| Характеристика  | L1-регуляризация (Lasso) | L2-регуляризация (Ridge) | Elastic Net |\n",
    "|----------------|----------------------|----------------------|-------------|\n",
    "| **Формула штрафа**  | \\\\( \\sum |w_j| \\\\)  | \\\\( \\sum w_j^2 \\\\)  | \\\\( \\lambda_1 \\sum |w_j| + \\lambda_2 \\sum w_j^2 \\\\)  |\n",
    "| **Эффект на веса**  | Обнуляет некоторые веса  | Уменьшает все веса  | Комбинирует оба |\n",
    "| **Применение**  | Если важен **отбор признаков**  | Если важна **стабильность модели**  | Если **данные коррелированы** |\n",
    "| **Использование**  | Логистическая регрессия, Lasso-регрессия  | Линейная регрессия, Ridge-регрессия  | Elastic Net |\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Итог**\n",
    "🔹 **L1-регуляризация (Lasso)** → Отбрасывает ненужные признаки.  \n",
    "🔹 **L2-регуляризация (Ridge)** → Сглаживает веса, делая модель стабильной.  \n",
    "🔹 **Elastic Net** → Объединяет лучшее от L1 и L2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3be1843",
   "metadata": {},
   "source": [
    "# **ROC и AUC: Подробное объяснение**\n",
    "\n",
    "## **1. Что такое ROC-кривая?**\n",
    "**ROC (Receiver Operating Characteristic)** – это график, который показывает, насколько хорошо модель различает классы.  \n",
    "Он строится на основе **различных пороговых значений** для вероятностных предсказаний модели.\n",
    "\n",
    "### **Оси ROC-кривой:**\n",
    "- **Ось Y** – **True Positive Rate (TPR)** (чувствительность, полнота, recall):  \n",
    "  \\\\[\n",
    "  TPR = \\frac{TP}{TP + FN}\n",
    "  \\\\]  \n",
    "  Это доля **правильно найденных положительных классов**.\n",
    "\n",
    "- **Ось X** – **False Positive Rate (FPR)** (ложноположительная ошибка):  \n",
    "  \\\\[\n",
    "  FPR = \\frac{FP}{FP + TN}\n",
    "  \\\\]  \n",
    "  Это доля **объектов класса 0, которые модель ошибочно отнесла к классу 1**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Как строится ROC-кривая?**\n",
    "1. **Модель предсказывает вероятности принадлежности к классу 1** (например, от 0 до 1).\n",
    "2. **Меняется порог классификации**:\n",
    "   - Если порог = **0**, все объекты классифицируются как **1** → TPR = 1, но FPR = 1.\n",
    "   - Если порог = **1**, все объекты классифицируются как **0** → TPR = 0, но FPR = 0.\n",
    "   - При промежуточных значениях порога TPR и FPR изменяются.\n",
    "3. **Рисуется кривая** (FPR по X, TPR по Y).\n",
    "\n",
    "📌 **Идеальная модель** будет **максимально далеко от диагонали** (левый верхний угол).\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Что такое AUC (Area Under Curve)?**\n",
    "**AUC (Area Under Curve)** – это **площадь под ROC-кривой**.  \n",
    "Она показывает **вероятность того, что случайный объект класса 1 будет классифицирован выше, чем случайный объект класса 0**.\n",
    "\n",
    "### **Интерпретация AUC:**\n",
    "| **AUC** | **Интерпретация** |\n",
    "|---------|------------------|\n",
    "| **1.0** | Идеальная модель (всегда верно определяет классы). |\n",
    "| **0.9 - 1.0** | Отличная модель. |\n",
    "| **0.8 - 0.9** | Хорошая модель. |\n",
    "| **0.7 - 0.8** | Средняя модель. |\n",
    "| **0.5 - 0.7** | Плохая модель. |\n",
    "| **0.5** | Модель случайного угадывания. |\n",
    "| **< 0.5** | Модель хуже случайного угадывания. |\n",
    "\n",
    "📌 **Чем выше AUC, тем лучше модель.**\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Пример расчета ROC и AUC на Python**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2128e3b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# 1. Генерация данных\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "# 2. Разделение на train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 3. Обучение модели\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 4. Предсказание вероятностей\n",
    "y_probs = model.predict_proba(X_test)[:, 1]  # Вероятности для класса 1\n",
    "\n",
    "# 5. Вычисление ROC-кривой\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# 6. Визуализация\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(fpr, tpr, color='blue', label=f'ROC-кривая (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label=\"Случайное угадывание\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC-кривая\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfe6b5e",
   "metadata": {},
   "source": [
    "📌 **На графике:**  \n",
    "- Синяя линия – ROC-кривая модели.  \n",
    "- Серая пунктирная линия – случайное угадывание.  \n",
    "- Чем **выше** синяя кривая, тем лучше.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Когда AUC-ROC не подходит?**\n",
    "Хотя ROC-AUC – мощная метрика, есть случаи, когда лучше использовать **Precision-Recall AUC**:\n",
    "1. **Несбалансированные данные (редкие события)** – ROC-AUC может быть высок, но модель все равно плохо классифицирует редкий класс.\n",
    "2. **Если важно минимизировать FP или FN** – лучше использовать Precision-Recall.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Итог**\n",
    "✅ **ROC-кривая** – показывает, как меняется TPR/FPR при изменении порога.  \n",
    "✅ **AUC (Area Under Curve)** – вероятность, что модель правильно различит классы.  \n",
    "✅ **AUC = 1** – идеальная модель, **AUC = 0.5** – случайное угадывание.  \n",
    "✅ **Использовать AUC-ROC, если классы сбалансированы, иначе Precision-Recall AUC.**\n",
    "\n",
    "🚀 **Если есть вопросы или нужно больше примеров – напиши!** 😊"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0cf2fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
